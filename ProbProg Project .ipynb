{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Starting with 30 sentences on various subjects (science, geography, art), we wish to output 10 topics of length 10 using LDA. A few of the sentences I used are:\n",
    "\n",
    "- Europe's longest river in terms of discharge and drainage basin is the Volga.\n",
    "- Mathematical structures are good models of real phenomena.\n",
    "- The Western tradition of sculpture began in ancient Greece.\n",
    "\n",
    "The first step is preprocessing the data (removing punctuation, stemming the words, changing uppercase characters into lowercase and removing stopwords). Afterwards, I created a dictionary to keep track of each unique word, and replaced each word with its corresponding number from the dictionary. The model for LDA was built as specified in the project's description, using the preprocessed documents as data. \n",
    "\n",
    "### Extras\n",
    "\n",
    "1. Can the topic model be used to define a topic-based similarity measure between documents? \n",
    "\n",
    "The Jensen Shannon Divergence is a method of measuring the similarity between two probability distributions. The square root of the Jensenâ€“Shannon divergence is a metric often referred to as Jensen-Shannon distance (JSD). It is derived from another measure of statistical distance called the Kullback-Leiber Divergence (KLD), but it is symmetric and it always has a finite value. \n",
    "[Source](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)\n",
    "\n",
    "To calculate the similarity between the topics of the two documents i and j, we can calculate:\n",
    "\n",
    "$$JSD(\\theta(i), \\theta(j))$$\n",
    "\n",
    "The value will be in the [0,1] interval, 0 meaning no similarity, and 1 meaning identical topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 10000 of 10000 complete in 132.6 sec[8 9 9 7 9 2 9 9]\n",
      "[6 2 6 7 3 1 3]\n",
      "[4 3 8 5 3 7 3 3 7 1]\n",
      "[0 4 1 1 3 1 0]\n",
      "[7 3 7 6 7 9]\n",
      "[7 1 9 7 3]\n",
      "[0 5 1 0 6 0]\n",
      "[1 2 4 1 8 4 3 5]\n",
      "[8 5 4 6 2 5 6 2 4 5 2 5 1]\n",
      "[7 8 7 2 2 7 8]\n",
      "[9 9 1 7]\n",
      "[9 9 5 6 9 5]\n",
      "[6 4 6 9 1 8 7]\n",
      "[3 5 5 8 8 4]\n",
      "[2 8 4 5 5 7]\n",
      "[9 3 6 0 6]\n",
      "[5 6 5 3 9 5 6 8]\n",
      "[7 2 4 7 8 1]\n",
      "[4 5 2 2 7 7 4 3 2 4 7 7]\n",
      "[9 1 2 9 3 2 5 9 5 1 3]\n",
      "[7 1 4 9 1 6]\n",
      "[9 2 9 7 4 4 9 5 1]\n",
      "[7 7 0 1 4 7]\n",
      "[9 5 1 7 5 7 7]\n",
      "[5 0 9 5 7 8 8 8 8 3 8 9]\n",
      "[1 1 8 4 6 3]\n",
      "[8 4 2 8 9 7 2 8 0]\n",
      "[8 8 8 0]\n",
      "[3 7 9 5 2 9]\n",
      "[9 0 4 6 0 5]\n",
      "Topic 0: [(0.03249283914526559, 'program'), (0.02896571696280665, 'peopl'), (0.02636600157357746, 'print'), (0.02636418368754826, 'visual'), (0.022840816825061835, 'empir'), (0.021603370556204676, 'interpret'), (0.020770102839029327, 'scienc'), (0.02000519702070068, 'refer'), (0.019107024359427262, 'made'), (0.0177296909183632, 'moscow')]\n",
      "\n",
      "\n",
      "Topic 1: [(0.03618141643975489, 'phenomena'), (0.03363939113314517, 'econom'), (0.03247534819804568, 'oper'), (0.029764455769865455, 'accessori'), (0.02949021261564236, 'term'), (0.028490358653575916, 'life'), (0.025966684512303894, 'theoret'), (0.022921479608885586, 'major'), (0.022335593063342603, 'process'), (0.022166852901837782, 'peopl')]\n",
      "\n",
      "\n",
      "Topic 2: [(0.03674204053788786, 'also'), (0.029373891087871803, 'visual'), (0.028197238023577705, 'practic'), (0.028143060537244448, 'sourc'), (0.025584043045130993, 'artwork'), (0.02512252237050089, 'ha'), (0.025036020553918202, 'audienc'), (0.021961448834263386, 'interpret'), (0.021003824657654973, 'interest'), (0.020544269296488442, 'real')]\n",
      "\n",
      "\n",
      "Topic 3: [(0.03476485735005993, 'western'), (0.028845394475939565, 'trend'), (0.02757210631137922, 'basin'), (0.02718780530430363, 'made'), (0.026553916809644116, 'behavior'), (0.023320324739409405, 'shape'), (0.02273562185301643, 'interact'), (0.022263533916135735, 'devot'), (0.021557982606971338, 'three'), (0.021063862567139276, 'ancient')]\n",
      "\n",
      "\n",
      "Topic 4: [(0.033477515546181294, 'commonli'), (0.030631704012224648, 'conjectur'), (0.02993388518332271, 'appli'), (0.028564034722764194, 'societi'), (0.025635590580453217, 'danub'), (0.02422443145509984, 'eleven'), (0.023385441569801758, 'formal'), (0.02323296330292903, 'capit'), (0.022451660118350168, 'togeth'), (0.02197468831649247, 'dimension')]\n",
      "\n",
      "\n",
      "Topic 5: [(0.04464607686302821, 'materi'), (0.035330207381698286, 'twenti'), (0.03532983045307054, 'citi'), (0.031935863422943786, 'moscow'), (0.030915762482945634, 'second'), (0.0296122767470434, 'process'), (0.028452236164230267, 'switzerland'), (0.02778450167491657, 'compound'), (0.023077936659234088, 'disciplin'), (0.0224747638515579, 'studi')]\n",
      "\n",
      "\n",
      "Topic 6: [(0.033032038236529455, 'volga'), (0.028387444131012046, 'togeth'), (0.025313948198241017, 'regard'), (0.025093185591346264, 'longest'), (0.02215659837317152, 'branch'), (0.022127088429651613, 'structur'), (0.021831148965731895, 'new'), (0.021819558558670144, 'text'), (0.01988565425431818, 'religi'), (0.0194879795079301, 'econom')]\n",
      "\n",
      "\n",
      "Topic 7: [(0.05236221811067574, 'tradit'), (0.027994684932901255, 'religi'), (0.026664737428934476, 'commonli'), (0.024030070862284477, 'sound'), (0.022195124458941312, 'phenomena'), (0.021781016989903326, 'econom'), (0.02141457252376681, 'peopl'), (0.02082794149721695, 'field'), (0.020634749814346615, 'mani'), (0.020175393152462882, 'print')]\n",
      "\n",
      "\n",
      "Topic 8: [(0.03571199787046906, 'visual'), (0.03441021130785687, 'theoret'), (0.02935638052568404, 'peopl'), (0.028601556999277612, 'creat'), (0.022272922904057274, 'rhine'), (0.02078056163643451, 'empti'), (0.020718639069566315, 'sourc'), (0.020682196376655596, 'cosmolog'), (0.02025373254759891, 'north'), (0.019676100226196015, 'combin')]\n",
      "\n",
      "\n",
      "Topic 9: [(0.04057785374240308, 'societi'), (0.027795275503307227, 'set'), (0.02681176847752558, 'base'), (0.02611540386401746, 'medicin'), (0.025538284595992892, 'appli'), (0.02538190078824937, 'longest'), (0.024756438146436313, 'program'), (0.022199329593483388, 'pattern'), (0.022148740240847037, 'twenti'), (0.02194305457583984, 'netherland')]\n",
      "\n",
      "\n",
      "JSD:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-c1778a71ebe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'JSD:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mJSD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-157-c1778a71ebe5>\u001b[0m in \u001b[0;36mJSD\u001b[1;34m(p, q)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;31m# entropy measure in scipy is implemented using the KLD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# compute Jensen Shannon Divergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mdivergence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m# compute the Jensen Shannon Distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(pk, qk, base)\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m         \u001b[0mqk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2512\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2513\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"qk and pk must have same length.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2514\u001b[0m         \u001b[0mqk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mqk\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import scipy\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#read 30 documents from input file\n",
    "documents = np.zeros([30,1], dtype=object)\n",
    "index = 0\n",
    "f = open (\"docs.txt\",\"r\")\n",
    "f1 = f.readlines()\n",
    "for x in f1:\n",
    "    documents[index] = x\n",
    "    index = index+1\n",
    "    \n",
    "#Preprocessing \n",
    "#remove \\n from documents\n",
    "for doc in documents:\n",
    "    doc[-1] = doc[-1].strip()\n",
    "#remove punctuation \n",
    "for index in range(len(documents)):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    documents[index] = str(documents[index]).translate(translator) \n",
    "#stem the words and make lowercase\n",
    "ps = PorterStemmer()\n",
    "dataset = [[ps.stem(word.lower()) for word in document[0].split(\" \")] for document in documents]\n",
    "dataset = [' '.join(fragment) for fragment in dataset]\n",
    "#remove stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_documents = []\n",
    "for document in dataset:\n",
    "    doc = []\n",
    "    for w in document.split(\" \"):\n",
    "        if not w in stop_words:\n",
    "            doc.append(w)\n",
    "    filtered_documents.append(' '.join(doc)) \n",
    "\n",
    "#create dictionary\n",
    "count = 0\n",
    "dictionary = {}\n",
    "for document in filtered_documents: \n",
    "    split_document = document.split(' ')\n",
    "    for word in split_document: \n",
    "        if (word not in dictionary.keys() and (not word.isspace()) and word):\n",
    "            dictionary[word] = count\n",
    "            count = count+1\n",
    "            \n",
    "#replace each word with the number from the dictionary\n",
    "data = [] \n",
    "for document in filtered_documents:\n",
    "    document_data = []\n",
    "    split_document = document.split(' ')\n",
    "    for word in split_document:\n",
    "        if ((not word.isspace()) and word):\n",
    "            document_data.append(dictionary[word])\n",
    "    data.append(document_data) \n",
    "#Show data     \n",
    "#for index in range(len(data)):\n",
    "    #print (data[index])\n",
    "\n",
    "K, N, D = 10, len(dictionary), len(data) # number of topics, words, documents\n",
    "\n",
    "\"\"\"the model trains to output psi(the distribution of words for each topic K) \n",
    "and phi (the distribution of topics for each document i)\"\"\"\n",
    "alpha = np.ones(K)\n",
    "#alpha = prior concentration parameter of the per-document topic distribution\n",
    "beta = np.ones(N)\n",
    "#beta = prior concentration parameter of the per-topic word distribution\n",
    "\n",
    "theta = pm.Container([pm.CompletedDirichlet(\"theta_%s\" % i, pm.Dirichlet(\"ptheta_%s\" % i, theta=alpha)) for i in range(N)])\n",
    "phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" % k, pm.Dirichlet(\"pphi_%s\" % k, theta=beta)) for k in range(K)])\n",
    "\n",
    "#theta(i) = topic distribution for document i\n",
    "#phi(k) = word distribution for topic k\n",
    "#theta, phi are Dirichlet distributions       \n",
    "Wd = [len(doc) for doc in data]\n",
    "Z = pm.Container([pm.Categorical(\"z_%i\" % d,\n",
    "                                 p=theta[d],\n",
    "                                 size=Wd[d],\n",
    "                                 value=np.random.randint(K,size=Wd[d]))\n",
    "                               for d in range(D)])\n",
    "#z(i,j) is the topic assignment for w(i,j)\n",
    "W = pm.Container([pm.Categorical(\"w_%i,%i\" % (d,i),\n",
    "                                p=pm.Lambda(\"phi_z_%i_%i\" % (d,i), \n",
    "                                        lambda z=Z[d][i], phi=phi: phi[z]),\n",
    "                                value=data[d][i],\n",
    "                                observed=True)\n",
    "                               for d in range(D) for i in range(Wd[d])])\n",
    "#w(i,j) is the j-th word of the i-th document\n",
    "        \n",
    "model = pm.Model([theta, phi, Z, W])\n",
    "mcmc = pm.MCMC(model)\n",
    "mcmc.sample(10000, 1000)\n",
    "\n",
    "#show the topic assignment for each word, using the last trace  \n",
    "for d in range(D):  \n",
    "    print(mcmc.trace('z_%i'%d)[8999])  \n",
    "    \n",
    "def sortFirst(val):\n",
    "    return val[0]\n",
    "\n",
    "\n",
    "for j in range(K):\n",
    "    t = (mcmc.trace('phi_%s'%j)[8999])\n",
    "    li = list(zip(t[0], (list(dictionary.keys()))))\n",
    "    li.sort(key=sortFirst, reverse=True)\n",
    "    print (\"Topic %i:\" %j, li[:10])\n",
    "    print ('\\n')\n",
    "\n",
    "def JSD(p, q):\n",
    "    \"\"\"\n",
    "    method to compute the Jenson-Shannon Distance \n",
    "    between two probability distributions\n",
    "    \"\"\"\n",
    "    # calculate m\n",
    "    m = (p + q) / 2\n",
    "    # entropy measure in scipy is implemented using the KLD\n",
    "    # compute Jensen Shannon Divergence\n",
    "    divergence = (scipy.stats.entropy(p, m) + scipy.stats.entropy(q, m)) / 2\n",
    "\n",
    "    # compute the Jensen Shannon Distance\n",
    "    distance = np.sqrt(divergence)\n",
    "\n",
    "    return distance \n",
    "\n",
    "print('JSD:')\n",
    "print (JSD(theta[1], theta[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What about a new document? How can topics be assigned to it?\n",
    "\n",
    "We take a new document, add it to the corpus, and then run Gibbs sampling just on the words in that new document, keeping the topic assignments of the old documents the same.\n",
    "I used a news headline dataset from Kaggle.\n",
    "[Source](https://www.kaggle.com/ibadia/dawn-news-headlines/data)\n",
    "\n",
    "New document examples:\n",
    "- As US foreign policy remains shrouded in uncertainty, its best Pakistan stays at a distance\n",
    "- Editorial: Inability of the Buzdar setup is commonly listed as a significant failure of Imran Khan's govt\n",
    "- Minister opens dam in Balochistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headline\n",
      "0          Unprecedented briefing planned by Taliban\n",
      "1        Economic decisions taken by cabinet ignored\n",
      "2  Crisis called Pakistan`s Katrina Kerry urges w...\n",
      "3       The unbearable lightness of Pakistan cricket\n",
      "4           Pakistan cricket dealt another hard blow\n",
      "Topic: 0 \n",
      " 0.029*\"kashmir\" + 0.028*\"fata\" + 0.027*\"amid\" + 0.024*\"plea\" + 0.021*\"musharraf\" + 0.018*\"gets\" + 0.017*\"protest\" + 0.016*\"census\" + 0.016*\"year\" + 0.015*\"case\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      " 0.107*\"killed\" + 0.067*\"attack\" + 0.038*\"militants\" + 0.026*\"blast\" + 0.022*\"courts\" + 0.021*\"quetta\" + 0.020*\"suicide\" + 0.020*\"indian\" + 0.017*\"kills\" + 0.017*\"drone\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      " 0.062*\"sharif\" + 0.056*\"says\" + 0.049*\"army\" + 0.033*\"balochistan\" + 0.028*\"chief\" + 0.025*\"security\" + 0.022*\"cabinet\" + 0.020*\"cpec\" + 0.020*\"meeting\" + 0.020*\"issue\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      " 0.100*\"govt\" + 0.037*\"sindh\" + 0.032*\"power\" + 0.026*\"cases\" + 0.022*\"crisis\" + 0.020*\"body\" + 0.019*\"parties\" + 0.018*\"budget\" + 0.017*\"punjab\" + 0.016*\"trade\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      " 0.108*\"pakistan\" + 0.058*\"talks\" + 0.050*\"india\" + 0.036*\"afghan\" + 0.028*\"peace\" + 0.027*\"rejects\" + 0.024*\"taliban\" + 0.017*\"afghanistan\" + 0.017*\"says\" + 0.015*\"town\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      " 0.061*\"opposition\" + 0.034*\"minister\" + 0.030*\"dead\" + 0.030*\"kabul\" + 0.027*\"seeks\" + 0.026*\"pakistan\" + 0.025*\"money\" + 0.024*\"help\" + 0.023*\"bilawal\" + 0.023*\"shot\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      " 0.048*\"karachi\" + 0.032*\"punjab\" + 0.028*\"shahbaz\" + 0.024*\"islamabad\" + 0.022*\"poll\" + 0.022*\"nisar\" + 0.019*\"vows\" + 0.017*\"delay\" + 0.016*\"terrorists\" + 0.016*\"bail\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      " 0.055*\"told\" + 0.046*\"nawaz\" + 0.044*\"asks\" + 0.039*\"sharifs\" + 0.033*\"govt\" + 0.021*\"support\" + 0.018*\"plans\" + 0.018*\"waziristan\" + 0.017*\"media\" + 0.016*\"peshawar\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      " 0.077*\"imran\" + 0.031*\"case\" + 0.029*\"court\" + 0.024*\"today\" + 0.023*\"ties\" + 0.021*\"indian\" + 0.018*\"govt\" + 0.018*\"death\" + 0.017*\"report\" + 0.017*\"arabia\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      " 0.038*\"probe\" + 0.036*\"zardari\" + 0.035*\"case\" + 0.034*\"held\" + 0.034*\"wants\" + 0.026*\"polls\" + 0.024*\"imran\" + 0.023*\"tells\" + 0.021*\"orders\" + 0.021*\"anti\"\n",
      "\n",
      "\n",
      "Testing the model on a new document:\n",
      "As US foreign policy remains shrouded in uncertainty, its best Pakistan stays at a distance\n",
      "Score: 0.3554139733314514\t Topic: 0.055*\"told\" + 0.046*\"nawaz\" + 0.044*\"asks\" + 0.039*\"sharifs\" + 0.033*\"govt\" + 0.021*\"support\" + 0.018*\"plans\" + 0.018*\"waziristan\" + 0.017*\"media\" + 0.016*\"peshawar\"\n",
      "\n",
      "\n",
      "Score: 0.19129537045955658\t Topic: 0.100*\"govt\" + 0.037*\"sindh\" + 0.032*\"power\" + 0.026*\"cases\" + 0.022*\"crisis\" + 0.020*\"body\" + 0.019*\"parties\" + 0.018*\"budget\" + 0.017*\"punjab\" + 0.016*\"trade\"\n",
      "\n",
      "\n",
      "Score: 0.18725673854351044\t Topic: 0.048*\"karachi\" + 0.032*\"punjab\" + 0.028*\"shahbaz\" + 0.024*\"islamabad\" + 0.022*\"poll\" + 0.022*\"nisar\" + 0.019*\"vows\" + 0.017*\"delay\" + 0.016*\"terrorists\" + 0.016*\"bail\"\n",
      "\n",
      "\n",
      "Score: 0.16601262986660004\t Topic: 0.108*\"pakistan\" + 0.058*\"talks\" + 0.050*\"india\" + 0.036*\"afghan\" + 0.028*\"peace\" + 0.027*\"rejects\" + 0.024*\"taliban\" + 0.017*\"afghanistan\" + 0.017*\"says\" + 0.015*\"town\"\n",
      "\n",
      "\n",
      "Score: 0.016671523451805115\t Topic: 0.029*\"kashmir\" + 0.028*\"fata\" + 0.027*\"amid\" + 0.024*\"plea\" + 0.021*\"musharraf\" + 0.018*\"gets\" + 0.017*\"protest\" + 0.016*\"census\" + 0.016*\"year\" + 0.015*\"case\"\n",
      "\n",
      "\n",
      "Score: 0.016670994460582733\t Topic: 0.061*\"opposition\" + 0.034*\"minister\" + 0.030*\"dead\" + 0.030*\"kabul\" + 0.027*\"seeks\" + 0.026*\"pakistan\" + 0.025*\"money\" + 0.024*\"help\" + 0.023*\"bilawal\" + 0.023*\"shot\"\n",
      "\n",
      "\n",
      "Score: 0.016670532524585724\t Topic: 0.077*\"imran\" + 0.031*\"case\" + 0.029*\"court\" + 0.024*\"today\" + 0.023*\"ties\" + 0.021*\"indian\" + 0.018*\"govt\" + 0.018*\"death\" + 0.017*\"report\" + 0.017*\"arabia\"\n",
      "\n",
      "\n",
      "Score: 0.016669869422912598\t Topic: 0.038*\"probe\" + 0.036*\"zardari\" + 0.035*\"case\" + 0.034*\"held\" + 0.034*\"wants\" + 0.026*\"polls\" + 0.024*\"imran\" + 0.023*\"tells\" + 0.021*\"orders\" + 0.021*\"anti\"\n",
      "\n",
      "\n",
      "Score: 0.01666925475001335\t Topic: 0.062*\"sharif\" + 0.056*\"says\" + 0.049*\"army\" + 0.033*\"balochistan\" + 0.028*\"chief\" + 0.025*\"security\" + 0.022*\"cabinet\" + 0.020*\"cpec\" + 0.020*\"meeting\" + 0.020*\"issue\"\n",
      "\n",
      "\n",
      "Score: 0.01666911505162716\t Topic: 0.107*\"killed\" + 0.067*\"attack\" + 0.038*\"militants\" + 0.026*\"blast\" + 0.022*\"courts\" + 0.021*\"quetta\" + 0.020*\"suicide\" + 0.020*\"indian\" + 0.017*\"kills\" + 0.017*\"drone\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "documents = pd.read_csv(r\"C:\\Users\\Asus\\Desktop\\pp\\categories_data.csv\")\n",
    "\n",
    "# Ignore columns that we won't use, we only keep the news headlines\n",
    "documents = documents.drop(columns=['PageType', 'Link_paper', 'Date','Link_news'], axis=1)\n",
    "# See a few headlines from the document\n",
    "print(documents.head())\n",
    "# Preprocess the text, remove stopwords and short words\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for i in gensim.utils.simple_preprocess(text):\n",
    "        if i not in gensim.parsing.preprocessing.STOPWORDS and len(i) > 3:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "processed_docs = documents['Headline'].map(preprocess)\n",
    "#processed_docs[:10]\n",
    "\n",
    "#Bag of Words on the dataset\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#print(bow_corpus[2700])\n",
    "\n",
    "# Run LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\n {}'.format(idx, topic))\n",
    "    print('\\n')\n",
    "    \n",
    "# Classifying a sample document \n",
    "#print(processed_docs[1563])\n",
    "#for index, score in sorted(lda_model[bow_corpus[1563]], key=lambda tup: -1*tup[1]):\n",
    "    #print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    \n",
    "# Testing model on a new document\n",
    "unseen_document = 'As US foreign policy remains shrouded in uncertainty, its best Pakistan stays at a distance'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "print('Testing the model on a new document:')\n",
    "print (unseen_document)\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by score, it is most likely that the new sentence belongs to the first topic:\n",
    "\n",
    "0.055*\"told\" + 0.046*\"nawaz\" + 0.044*\"asks\" + 0.039*\"sharifs\" + 0.033*\"govt\" + 0.021*\"support\" + 0.018*\"plans\" + 0.018*\"waziristan\" + 0.017*\"media\" + 0.016*\"peshawar\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
